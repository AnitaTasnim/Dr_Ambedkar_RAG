# report_generator.py
import json
import sys
from collections import Counter
from pathlib import Path

RESULTS_FILE = Path("test_results.json")
OUT_MD = Path("evaluation_report.md")

# -----------------------
# Load JSON safely
# -----------------------
if not RESULTS_FILE.exists():
    print("ERROR: test_results.json not found. Run evaluation.py first.")
    sys.exit(1)

try:
    with open(RESULTS_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
except Exception as e:
    print("Failed to read test_results.json:", e)
    sys.exit(1)

# -----------------------
# Metric functions
# -----------------------
def failure_types_from_sample(result):
    failures = []
    if result.get("hit_rate", 0) < 0.7:
        failures.append("retrieval_misses")
    if result.get("precision@3", 0) < 0.6:
        failures.append("noisy_topk")
    if result.get("rougeL", 0) < 0.20:
        failures.append("poor_answer_overlap")
    if result.get("bleu", 0) < 0.04:
        failures.append("paraphrase_mismatch")
    if result.get("cosine", 0) < 0.45:
        failures.append("semantic_mismatch")
    return failures

def score_config(r):
    return (
        3 * r.get("hit_rate", 0) +
        2 * r.get("mrr", 0) +
        2 * r.get("precision@3", 0) +
        1 * r.get("rougeL", 0) +
        1 * r.get("cosine", 0)
    )

# -----------------------
# Compute best config
# -----------------------
best = max(data, key=score_config)
mean_hit = sum(r["hit_rate"] for r in data) / len(data)

# Failures aggregation
all_failures = []
for r in data:
    all_failures += failure_types_from_sample(r)
failure_counts = Counter(all_failures)

# -----------------------
# Write Markdown Report
# -----------------------
md = []

md.append("# AmbedkarGPT — Evaluation Report\n")
md.append("Auto-generated by `report_generator.py`.\n")

# 1) Best chunking strategy
md.append("## 1) Best Chunking Strategy\n")
md.append(f"- **chunk_size:** {best['chunk_size']}\n")
md.append(f"- **overlap:** {best['overlap']}\n")
md.append(f"- **Weighted performance score:** {score_config(best):.4f}\n")
md.append("Reason: Highest combined retrieval + semantic + answer metrics.\n")

# 2) Accuracy
md.append("\n## 2) Current System Accuracy\n")
md.append(f"- **Hit Rate (best config):** {best['hit_rate']:.3f}\n")
md.append(f"- **Mean Hit Rate (all configs):** {mean_hit:.3f}\n")

# 3) Failure types
md.append("\n## 3) Most Common Failure Types\n")
if failure_counts:
    for name, cnt in failure_counts.most_common():
        md.append(f"- {name}: {cnt} occurrences\n")
else:
    md.append("- No detected failures\n")

# 4) Improvements
md.append("\n## 4) Recommended Improvements\n")
md.append("1. Stronger embedding model or fine-tuned domain embeddings.\n")
md.append("2. Hybrid retrieval (BM25 + dense).\n")
md.append("3. Cross-encoder reranking.\n")
md.append("4. Semantic chunk boundaries instead of fixed window.\n")
md.append("5. More grounded answer-extraction prompting.\n")

# 5) Summary Table
md.append("\n## 5) Summary Table\n")
md.append("|chunk_size|overlap|hit_rate|mrr|precision@3|rougeL|bleu|cosine|\n")
md.append("|---|---|---:|---:|---:|---:|---:|---:|\n")

for r in data:
    md.append(
        f"|{r['chunk_size']}|{r['overlap']}|"
        f"{r['hit_rate']:.3f}|{r['mrr']:.3f}|{r['precision@3']:.3f}|"
        f"{r['rougeL']:.3f}|{r['bleu']:.3f}|{r['cosine']:.3f}|\n"
    )

# Write file
OUT_MD.write_text("".join(md), encoding="utf-8")
print(f"[OK] Evaluation report generated → {OUT_MD.resolve()}")
